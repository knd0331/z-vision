"""
Z-Vision - Z-Image-Turbo ê¸°ë°˜ ì´ë¯¸ì§€ ìƒì„± ì›¹ UI (í†µí•© ë²„ì „)

ì§€ì› ë°±ì—”ë“œ:
    - MLX (Apple Silicon ìµœì í™”)
    - CUDA (NVIDIA GPU)
    - MPS (Apple Metal - PyTorch)
    - CPU (í´ë°±)

ì‚¬ìš©ë²•:
    python app.py

ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:7860 ì ‘ì†
"""

import os
import platform
import time
from datetime import datetime
from pathlib import Path

import gradio as gr
from PIL import Image

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
OUTPUT_DIR = Path("outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

# ì „ì—­ ë³€ìˆ˜
_backend = None
_model = None
_pipeline = None


def detect_backend() -> str:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ ë°±ì—”ë“œë¥¼ ê°ì§€."""
    # 1. Apple Siliconì—ì„œ MLX ìš°ì„  ì²´í¬
    if platform.system() == "Darwin" and platform.machine() == "arm64":
        try:
            import mlx.core  # noqa: F401
            from mflux.models.z_image.variants.turbo.z_image_turbo import ZImageTurbo  # noqa: F401
            return "mlx"
        except ImportError:
            pass  # MLX ì—†ìœ¼ë©´ PyTorchë¡œ í´ë°±

    # 2. PyTorch ê¸°ë°˜ ë°±ì—”ë“œ ì²´í¬
    try:
        import torch
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    except ImportError:
        pass

    # 3. ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ None
    return None


def get_backend_info(backend: str) -> dict:
    """ë°±ì—”ë“œë³„ ì„¤ì • ì •ë³´ ë°˜í™˜."""
    configs = {
        "mlx": {
            "name": "MLX (Apple Silicon)",
            "emoji": "ğŸ",
            "default_steps": 4,
            "default_size": 512,
            "max_size": 1536,
            "step_info": "4 ê¶Œì¥ (Turbo ìµœì í™”)",
        },
        "cuda": {
            "name": "CUDA (NVIDIA GPU)",
            "emoji": "ğŸ®",
            "default_steps": 6,
            "default_size": 1024,
            "max_size": 2048,
            "step_info": "6-8 ê¶Œì¥",
        },
        "mps": {
            "name": "MPS (Apple Metal)",
            "emoji": "ğŸ",
            "default_steps": 6,
            "default_size": 512,
            "max_size": 1024,
            "step_info": "6 ê¶Œì¥ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)",
        },
        "cpu": {
            "name": "CPU",
            "emoji": "ğŸ’»",
            "default_steps": 4,
            "default_size": 512,
            "max_size": 768,
            "step_info": "4 ê¶Œì¥ (ëŠë¦¼ ì£¼ì˜)",
        },
    }
    return configs.get(backend, configs["cpu"])


# ============================================================
# MLX Backend
# ============================================================

def get_mlx_model():
    """MFLUX ZImageTurbo ëª¨ë¸ ë¡œë“œ (lazy loading)."""
    global _model
    if _model is None:
        print("ğŸš€ Z-Image-Turbo ëª¨ë¸ ë¡œë”© ì¤‘ (MLX)...")
        from mflux.models.z_image.variants.turbo.z_image_turbo import ZImageTurbo
        _model = ZImageTurbo(quantize=8)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (MLX + 8-bit ì–‘ìí™”)")
    return _model


def generate_mlx(prompt: str, width: int, height: int, num_steps: int, seed: int) -> tuple[Image.Image, float]:
    """MLX ë°±ì—”ë“œë¡œ ì´ë¯¸ì§€ ìƒì„±."""
    import random
    model = get_mlx_model()

    if seed == -1:
        seed = random.randint(0, 2**32 - 1)

    result = model.generate_image(
        seed=seed,
        prompt=prompt,
        width=width,
        height=height,
        num_inference_steps=num_steps,
    )

    return result.image, result.generation_time, seed


# ============================================================
# PyTorch/Diffusers Backend
# ============================================================

def get_diffusers_pipeline(device: str):
    """Diffusers ZImagePipeline ë¡œë“œ (lazy loading)."""
    global _pipeline
    if _pipeline is None:
        print(f"ğŸš€ Z-Image-Turbo ëª¨ë¸ ë¡œë”© ì¤‘ (PyTorch/{device.upper()})...")
        import torch
        from diffusers import ZImagePipeline

        _pipeline = ZImagePipeline.from_pretrained(
            "Tongyi-MAI/Z-Image-Turbo",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
        )

        if device == "cuda":
            _pipeline.to("cuda")
            print("âœ… CUDA GPUì—ì„œ ì‹¤í–‰")
        elif device == "mps":
            _pipeline.to("mps")
            _pipeline.enable_attention_slicing()
            print("âœ… Apple MPSì—ì„œ ì‹¤í–‰ (attention slicing í™œì„±í™”)")
        else:
            _pipeline.to("cpu")
            _pipeline.enable_attention_slicing()
            print("âš ï¸ CPUì—ì„œ ì‹¤í–‰ (ëŠë¦´ ìˆ˜ ìˆìŒ)")

        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    return _pipeline


def generate_diffusers(prompt: str, width: int, height: int, num_steps: int, seed: int, device: str) -> tuple[Image.Image, float]:
    """PyTorch/Diffusers ë°±ì—”ë“œë¡œ ì´ë¯¸ì§€ ìƒì„±."""
    import torch
    pipe = get_diffusers_pipeline(device)

    if seed == -1:
        seed = torch.randint(0, 2**32, (1,)).item()

    generator = torch.Generator(device if device != "cpu" else "cpu").manual_seed(int(seed))

    start_time = time.time()
    result = pipe(
        prompt=prompt,
        width=width,
        height=height,
        num_inference_steps=num_steps,
        guidance_scale=0.0,  # Turbo ëª¨ë¸ì€ 0.0 í•„ìˆ˜
        generator=generator,
    )
    gen_time = time.time() - start_time

    return result.images[0], gen_time, seed


# ============================================================
# Unified Generation
# ============================================================

def generate_image(
    prompt: str,
    width: int,
    height: int,
    num_steps: int,
    seed: int,
    save_image: bool,
) -> tuple[Image.Image, str]:
    """í†µí•© ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜."""
    global _backend

    if not prompt.strip():
        return None, "âŒ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

    if _backend is None:
        return None, "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤. PyTorch ë˜ëŠ” MLXë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."

    try:
        print(f"ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (backend: {_backend})")

        if _backend == "mlx":
            image, gen_time, used_seed = generate_mlx(prompt, width, height, num_steps, seed)
        else:
            image, gen_time, used_seed = generate_diffusers(prompt, width, height, num_steps, seed, _backend)

        # ìƒíƒœ ë©”ì‹œì§€
        backend_info = get_backend_info(_backend)
        status = f"âœ… ìƒì„± ì™„ë£Œ! ({backend_info['emoji']} {_backend.upper()}, seed: {used_seed}, {gen_time:.1f}ì´ˆ)"

        # ì´ë¯¸ì§€ ì €ì¥
        if save_image:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = OUTPUT_DIR / f"zvision_{timestamp}_{used_seed}.png"
            image.save(filename)
            status += f"\nğŸ’¾ ì €ì¥ë¨: {filename}"

        return image, status

    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# ============================================================
# Gradio UI
# ============================================================

def create_ui():
    """Gradio UI ìƒì„±."""
    global _backend
    _backend = detect_backend()
    backend_info = get_backend_info(_backend) if _backend else {"name": "None", "emoji": "âŒ"}

    with gr.Blocks() as app:

        gr.HTML(f"""
        <div class="title">
            <h1>ğŸ¨ Z-Vision</h1>
            <p>Z-Image-Turbo AI ì´ë¯¸ì§€ ìƒì„±ê¸°</p>
            <p class="backend-info">{backend_info['emoji']} Backend: <strong>{backend_info['name'] if _backend else 'ì—†ìŒ'}</strong></p>
        </div>
        """)

        if _backend is None:
            gr.HTML("""
            <div class="error-box">
                <p>âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤!</p>
                <p>PyTorch ë˜ëŠ” MLXë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.</p>
            </div>
            """)

        with gr.Row():
            with gr.Column(scale=1):
                # ì…ë ¥ ì„¹ì…˜
                prompt = gr.Textbox(
                    label="í”„ë¡¬í”„íŠ¸",
                    placeholder="ìƒì„±í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”...",
                    lines=4,
                    max_lines=8,
                )

                with gr.Row():
                    width = gr.Slider(
                        label="ë„ˆë¹„",
                        minimum=512,
                        maximum=backend_info.get("max_size", 1536) if _backend else 1024,
                        value=backend_info.get("default_size", 512) if _backend else 512,
                        step=64,
                    )
                    height = gr.Slider(
                        label="ë†’ì´",
                        minimum=512,
                        maximum=backend_info.get("max_size", 1536) if _backend else 1024,
                        value=backend_info.get("default_size", 512) if _backend else 512,
                        step=64,
                    )

                with gr.Row():
                    num_steps = gr.Slider(
                        label="ìŠ¤í… ìˆ˜",
                        minimum=2,
                        maximum=10,
                        value=backend_info.get("default_steps", 4) if _backend else 4,
                        step=1,
                        info=backend_info.get("step_info", "") if _backend else "",
                    )
                    seed = gr.Number(
                        label="ì‹œë“œ",
                        value=-1,
                        precision=0,
                        info="-1 = ëœë¤",
                    )

                save_image = gr.Checkbox(
                    label="ì´ë¯¸ì§€ ìë™ ì €ì¥",
                    value=True,
                    info=f"ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR.absolute()}",
                )

                generate_btn = gr.Button(
                    "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±",
                    variant="primary",
                    size="lg",
                    interactive=_backend is not None,
                )

            with gr.Column(scale=1):
                # ì¶œë ¥ ì„¹ì…˜
                output_image = gr.Image(
                    label="ìƒì„±ëœ ì´ë¯¸ì§€",
                    type="pil",
                    height=512,
                )
                status = gr.Textbox(
                    label="ìƒíƒœ",
                    interactive=False,
                )

        # ì˜ˆì œ
        gr.Examples(
            examples=[
                ["A majestic mountain landscape at sunset with snow-capped peaks and a crystal clear lake reflection"],
                ["ê·€ì—¬ìš´ í•˜ì–€ ê³ ì–‘ì´ê°€ ì°½ê°€ì—ì„œ ë‚®ì ì„ ìê³  ìˆëŠ” ëª¨ìŠµ, ë”°ëœ»í•œ í–‡ì‚´"],
                ["Cyberpunk city street at night, neon lights, rain reflections, cinematic atmosphere"],
                ["í•œë³µì„ ì…ì€ ì—¬ì„±ì´ ë²šê½ƒ ë‚˜ë¬´ ì•„ë˜ ì„œ ìˆëŠ” ë™ì–‘í™” ìŠ¤íƒ€ì¼"],
                ["Delicious Korean bibimbap in a stone pot, food photography, top view"],
            ],
            inputs=[prompt],
            label="ì˜ˆì œ í”„ë¡¬í”„íŠ¸",
        )

        # ì´ë²¤íŠ¸ ì—°ê²°
        generate_btn.click(
            fn=generate_image,
            inputs=[prompt, width, height, num_steps, seed, save_image],
            outputs=[output_image, status],
        )

        # Enter í‚¤ë¡œ ìƒì„±
        prompt.submit(
            fn=generate_image,
            inputs=[prompt, width, height, num_steps, seed, save_image],
            outputs=[output_image, status],
        )

        gr.HTML("""
        <div class="footer">
            <p>Powered by <a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo" target="_blank">Z-Image-Turbo</a> |
            <a href="https://github.com/filipstrand/mflux" target="_blank">MFLUX</a> (MLX) |
            <a href="https://huggingface.co/docs/diffusers" target="_blank">Diffusers</a> (PyTorch)</p>
        </div>
        """)

    return app


if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ¨ Z-Vision ì‹œì‘")
    print("=" * 50)

    backend = detect_backend()
    if backend:
        info = get_backend_info(backend)
        print(f"{info['emoji']} ê°ì§€ëœ ë°±ì—”ë“œ: {info['name']}")
    else:
        print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ ì—†ìŒ")

    app = create_ui()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        theme=gr.themes.Soft(),
        css="""
        .title { text-align: center; margin-bottom: 1rem; }
        .title .backend-info { font-size: 0.9em; color: #666; margin-top: 0.5rem; }
        .footer { text-align: center; margin-top: 1rem; opacity: 0.7; }
        .error-box { background: #fee; border: 1px solid #fcc; padding: 1rem; border-radius: 8px; margin: 1rem 0; text-align: center; }
        """,
        head="<title>Z-Vision</title>",
    )
