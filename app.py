"""
Z-Vision - Z-Image-Turbo ê¸°ë°˜ ì´ë¯¸ì§€ ìƒì„± ì›¹ UI (í†µí•© ë²„ì „)

ì§€ì› ë°±ì—”ë“œ:
    - MLX (Apple Silicon ìµœì í™”)
    - CUDA (NVIDIA GPU)
    - MPS (Apple Metal - PyTorch)
    - CPU (í´ë°±)

ì‚¬ìš©ë²•:
    python app.py

ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:7860 ì ‘ì†
"""

import os
import platform
import time
from datetime import datetime
from pathlib import Path

import gradio as gr
from PIL import Image

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
OUTPUT_DIR = Path("outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

# ì „ì—­ ë³€ìˆ˜
_backend = None
_model = None
_pipeline = None
_img2img_pipeline = None  # Image-to-Image íŒŒì´í”„ë¼ì¸
_is_generating = False
_cancel_requested = False


def detect_backend() -> str:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ ë°±ì—”ë“œë¥¼ ê°ì§€."""
    # 1. Apple Siliconì—ì„œ MLX ìš°ì„  ì²´í¬
    if platform.system() == "Darwin" and platform.machine() == "arm64":
        try:
            import mlx.core  # noqa: F401
            from mflux.models.z_image.variants.turbo.z_image_turbo import ZImageTurbo  # noqa: F401
            return "mlx"
        except ImportError:
            pass  # MLX ì—†ìœ¼ë©´ PyTorchë¡œ í´ë°±

    # 2. PyTorch ê¸°ë°˜ ë°±ì—”ë“œ ì²´í¬
    try:
        import torch
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    except ImportError:
        pass

    # 3. ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ None
    return None


def get_backend_info(backend: str) -> dict:
    """ë°±ì—”ë“œë³„ ì„¤ì • ì •ë³´ ë°˜í™˜."""
    configs = {
        "mlx": {
            "name": "MLX (Apple Silicon)",
            "emoji": "ğŸ",
            "default_steps": 4,
            "default_size": 512,
            "max_size": 1536,
            "step_info": "4 ê¶Œì¥ (Turbo ìµœì í™”)",
        },
        "cuda": {
            "name": "CUDA (NVIDIA GPU)",
            "emoji": "ğŸ®",
            "default_steps": 6,
            "default_size": 1024,
            "max_size": 2048,
            "step_info": "6-8 ê¶Œì¥",
        },
        "mps": {
            "name": "MPS (Apple Metal)",
            "emoji": "ğŸ",
            "default_steps": 6,
            "default_size": 512,
            "max_size": 1024,
            "step_info": "6 ê¶Œì¥ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)",
        },
        "cpu": {
            "name": "CPU",
            "emoji": "ğŸ’»",
            "default_steps": 4,
            "default_size": 512,
            "max_size": 768,
            "step_info": "4 ê¶Œì¥ (ëŠë¦¼ ì£¼ì˜)",
        },
    }
    return configs.get(backend, configs["cpu"])


# ============================================================
# MLX Backend
# ============================================================

def get_mlx_model():
    """MFLUX ZImageTurbo ëª¨ë¸ ë¡œë“œ (lazy loading)."""
    global _model
    if _model is None:
        print("ğŸš€ Z-Image-Turbo ëª¨ë¸ ë¡œë”© ì¤‘ (MLX)...")
        from mflux.models.z_image.variants.turbo.z_image_turbo import ZImageTurbo
        _model = ZImageTurbo(quantize=8)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (MLX + 8-bit ì–‘ìí™”)")
    return _model


def generate_mlx(prompt: str, width: int, height: int, num_steps: int, seed: int) -> tuple[Image.Image, float]:
    """MLX ë°±ì—”ë“œë¡œ ì´ë¯¸ì§€ ìƒì„±."""
    import random
    model = get_mlx_model()

    if seed == -1:
        seed = random.randint(0, 2**32 - 1)

    result = model.generate_image(
        seed=seed,
        prompt=prompt,
        width=width,
        height=height,
        num_inference_steps=num_steps,
    )

    return result.image, result.generation_time, seed


# ============================================================
# PyTorch/Diffusers Backend
# ============================================================

def get_diffusers_pipeline(device: str):
    """Diffusers ZImagePipeline ë¡œë“œ (lazy loading)."""
    global _pipeline
    if _pipeline is None:
        print(f"ğŸš€ Z-Image-Turbo ëª¨ë¸ ë¡œë”© ì¤‘ (PyTorch/{device.upper()})...")
        import torch
        from diffusers import ZImagePipeline

        _pipeline = ZImagePipeline.from_pretrained(
            "Tongyi-MAI/Z-Image-Turbo",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
        )

        if device == "cuda":
            _pipeline.to("cuda")
            print("âœ… CUDA GPUì—ì„œ ì‹¤í–‰")
        elif device == "mps":
            _pipeline.to("mps")
            _pipeline.enable_attention_slicing()
            print("âœ… Apple MPSì—ì„œ ì‹¤í–‰ (attention slicing í™œì„±í™”)")
        else:
            _pipeline.to("cpu")
            _pipeline.enable_attention_slicing()
            print("âš ï¸ CPUì—ì„œ ì‹¤í–‰ (ëŠë¦´ ìˆ˜ ìˆìŒ)")

        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    return _pipeline


def make_progress_callback(num_steps: int, progress_fn=None):
    """Diffusersìš© ì§„í–‰ë¥  ì½œë°± ìƒì„±."""
    def callback(pipeline, step, timestep, callback_kwargs):
        global _cancel_requested
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if progress_fn is not None:
            progress_fn((step + 1) / num_steps, desc=f"Step {step + 1}/{num_steps}")
        
        # ì·¨ì†Œ ìš”ì²­ í™•ì¸
        if _cancel_requested:
            pipeline._interrupt = True
        
        return callback_kwargs
    
    return callback


def generate_diffusers(prompt: str, width: int, height: int, num_steps: int, seed: int, device: str, progress_fn=None) -> tuple[Image.Image, float]:
    """PyTorch/Diffusers ë°±ì—”ë“œë¡œ ì´ë¯¸ì§€ ìƒì„±."""
    import torch
    pipe = get_diffusers_pipeline(device)

    if seed == -1:
        seed = torch.randint(0, 2**32, (1,)).item()

    generator = torch.Generator(device if device != "cpu" else "cpu").manual_seed(int(seed))

    # ì§„í–‰ë¥  ì½œë°± ìƒì„±
    callback = make_progress_callback(num_steps, progress_fn)

    start_time = time.time()
    result = pipe(
        prompt=prompt,
        width=width,
        height=height,
        num_inference_steps=num_steps,
        guidance_scale=0.0,  # Turbo ëª¨ë¸ì€ 0.0 í•„ìˆ˜
        generator=generator,
        callback_on_step_end=callback,
    )
    gen_time = time.time() - start_time

    return result.images[0], gen_time, seed


# ============================================================
# Image-to-Image Backend (Diffusers Only)
# ============================================================

def get_img2img_pipeline(device: str):
    """Diffusers ZImageImg2ImgPipeline ë¡œë“œ (lazy loading)."""
    global _img2img_pipeline
    if _img2img_pipeline is None:
        print(f"ğŸš€ Z-Image-Turbo Img2Img ëª¨ë¸ ë¡œë”© ì¤‘ (PyTorch/{device.upper()})...")
        import torch
        from diffusers import ZImageImg2ImgPipeline

        _img2img_pipeline = ZImageImg2ImgPipeline.from_pretrained(
            "Tongyi-MAI/Z-Image-Turbo",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
        )

        if device == "cuda":
            _img2img_pipeline.to("cuda")
            print("âœ… CUDA GPUì—ì„œ ì‹¤í–‰ (Img2Img)")
        elif device == "mps":
            _img2img_pipeline.to("mps")
            _img2img_pipeline.enable_attention_slicing()
            print("âœ… Apple MPSì—ì„œ ì‹¤í–‰ (Img2Img, attention slicing í™œì„±í™”)")
        else:
            _img2img_pipeline.to("cpu")
            _img2img_pipeline.enable_attention_slicing()
            print("âš ï¸ CPUì—ì„œ ì‹¤í–‰ (Img2Img, ëŠë¦´ ìˆ˜ ìˆìŒ)")

        print("âœ… Img2Img ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    return _img2img_pipeline


def generate_img2img(
    prompt: str,
    init_image: Image.Image,
    strength: float,
    num_steps: int,
    seed: int,
    device: str,
    progress_fn=None,
) -> tuple[Image.Image, float, int]:
    """PyTorch/Diffusers ë°±ì—”ë“œë¡œ Image-to-Image ìƒì„±."""
    import torch
    pipe = get_img2img_pipeline(device)

    if seed == -1:
        seed = torch.randint(0, 2**32, (1,)).item()

    generator = torch.Generator(device if device != "cpu" else "cpu").manual_seed(int(seed))

    # ì§„í–‰ë¥  ì½œë°± ìƒì„±
    callback = make_progress_callback(num_steps, progress_fn)

    start_time = time.time()
    result = pipe(
        prompt=prompt,
        image=init_image,
        strength=strength,
        num_inference_steps=num_steps,
        guidance_scale=0.0,  # Turbo ëª¨ë¸ì€ 0.0 í•„ìˆ˜
        generator=generator,
        callback_on_step_end=callback,
    )
    gen_time = time.time() - start_time

    return result.images[0], gen_time, seed


# ============================================================
# Unified Generation
# ============================================================

def cancel_generation():
    """ìƒì„± ì·¨ì†Œ ìš”ì²­."""
    global _cancel_requested, _is_generating
    if _is_generating:
        _cancel_requested = True
        return "â¹ï¸ ì·¨ì†Œ ìš”ì²­ë¨... í˜„ì¬ ìŠ¤í… ì™„ë£Œ í›„ ì¤‘ë‹¨ë©ë‹ˆë‹¤."
    return "ìƒì„± ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤."


def generate_image(
    prompt: str,
    width: int,
    height: int,
    num_steps: int,
    seed: int,
    save_image: bool,
    progress=gr.Progress(),
) -> tuple[Image.Image, str]:
    """í†µí•© ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜."""
    global _backend, _is_generating, _cancel_requested

    if not prompt.strip():
        return None, "âŒ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

    if _backend is None:
        return None, "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤. PyTorch ë˜ëŠ” MLXë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."

    if _is_generating:
        return None, "âš ï¸ ì´ë¯¸ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê±°ë‚˜ ì·¨ì†Œí•´ì£¼ì„¸ìš”."

    try:
        _is_generating = True
        _cancel_requested = False
        
        print(f"ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (backend: {_backend})")
        progress(0, desc="ìƒì„± ì‹œì‘...")

        if _backend == "mlx":
            # MLXëŠ” ì½œë°± ë¯¸ì§€ì›, ë‹¨ìˆœ ì§„í–‰ë¥  í‘œì‹œ
            progress(0.1, desc="MLX ìƒì„± ì¤‘... (ì§„í–‰ë¥  í‘œì‹œ ë¯¸ì§€ì›)")
            image, gen_time, used_seed = generate_mlx(prompt, width, height, num_steps, seed)
        else:
            # DiffusersëŠ” ì½œë°±ìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
            image, gen_time, used_seed = generate_diffusers(
                prompt, width, height, num_steps, seed, _backend,
                progress_fn=progress
            )

        # ì·¨ì†Œ í™•ì¸
        if _cancel_requested:
            _is_generating = False
            _cancel_requested = False
            return None, "â¹ï¸ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."

        # ìƒíƒœ ë©”ì‹œì§€
        backend_info = get_backend_info(_backend)
        status = f"âœ… ìƒì„± ì™„ë£Œ! ({backend_info['emoji']} {_backend.upper()}, seed: {used_seed}, {gen_time:.1f}ì´ˆ)"

        # ì´ë¯¸ì§€ ì €ì¥
        if save_image and image is not None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = OUTPUT_DIR / f"zvision_{timestamp}_{used_seed}.png"
            image.save(filename)
            status += f"\nğŸ’¾ ì €ì¥ë¨: {filename}"

        progress(1.0, desc="ì™„ë£Œ!")
        return image, status

    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    finally:
        _is_generating = False
        _cancel_requested = False


def generate_image_i2i(
    prompt: str,
    init_image: Image.Image,
    strength: float,
    num_steps: int,
    seed: int,
    save_image: bool,
    progress=gr.Progress(),
) -> tuple[Image.Image, str]:
    """í†µí•© Image-to-Image ìƒì„± í•¨ìˆ˜."""
    global _backend, _is_generating, _cancel_requested

    if not prompt.strip():
        return None, "âŒ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

    if init_image is None:
        return None, "âŒ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

    if _backend is None:
        return None, "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤."

    if _backend == "mlx":
        return None, "âŒ MLX ë°±ì—”ë“œëŠ” Image-to-Imageë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. PyTorch ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”."

    if _is_generating:
        return None, "âš ï¸ ì´ë¯¸ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê±°ë‚˜ ì·¨ì†Œí•´ì£¼ì„¸ìš”."

    try:
        _is_generating = True
        _cancel_requested = False

        print(f"ğŸ–¼ï¸ Image-to-Image ìƒì„± ì¤‘... (backend: {_backend}, strength: {strength})")
        progress(0, desc="Img2Img ìƒì„± ì‹œì‘...")

        image, gen_time, used_seed = generate_img2img(
            prompt, init_image, strength, num_steps, seed, _backend,
            progress_fn=progress
        )

        # ì·¨ì†Œ í™•ì¸
        if _cancel_requested:
            _is_generating = False
            _cancel_requested = False
            return None, "â¹ï¸ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."

        # ìƒíƒœ ë©”ì‹œì§€
        backend_info = get_backend_info(_backend)
        status = f"âœ… Img2Img ì™„ë£Œ! ({backend_info['emoji']} {_backend.upper()}, strength: {strength}, seed: {used_seed}, {gen_time:.1f}ì´ˆ)"

        # ì´ë¯¸ì§€ ì €ì¥
        if save_image and image is not None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = OUTPUT_DIR / f"zvision_i2i_{timestamp}_{used_seed}.png"
            image.save(filename)
            status += f"\nğŸ’¾ ì €ì¥ë¨: {filename}"

        progress(1.0, desc="ì™„ë£Œ!")
        return image, status

    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    finally:
        _is_generating = False
        _cancel_requested = False


# ============================================================
# Gradio UI
# ============================================================

def create_ui():
    """Gradio UI ìƒì„±."""
    global _backend
    _backend = detect_backend()
    backend_info = get_backend_info(_backend) if _backend else {"name": "None", "emoji": "âŒ"}

    # Image-to-Image ì§€ì› ì—¬ë¶€ (MLXëŠ” ë¯¸ì§€ì›)
    i2i_supported = _backend is not None and _backend != "mlx"

    with gr.Blocks() as app:

        gr.HTML(f"""
        <div class="title">
            <h1>ğŸ¨ Z-Vision</h1>
            <p>Z-Image-Turbo AI ì´ë¯¸ì§€ ìƒì„±ê¸°</p>
            <p class="backend-info">{backend_info['emoji']} Backend: <strong>{backend_info['name'] if _backend else 'ì—†ìŒ'}</strong></p>
        </div>
        """)

        if _backend is None:
            gr.HTML("""
            <div class="error-box">
                <p>âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤!</p>
                <p>PyTorch ë˜ëŠ” MLXë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.</p>
            </div>
            """)

        with gr.Tabs():
            # ============================================================
            # Tab 1: Text-to-Image
            # ============================================================
            with gr.TabItem("ğŸ¨ Text-to-Image"):
                with gr.Row():
                    with gr.Column(scale=1):
                        # ì…ë ¥ ì„¹ì…˜
                        prompt_t2i = gr.Textbox(
                            label="í”„ë¡¬í”„íŠ¸",
                            placeholder="ìƒì„±í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”...",
                            lines=4,
                            max_lines=8,
                        )

                        with gr.Row():
                            width_t2i = gr.Slider(
                                label="ë„ˆë¹„",
                                minimum=512,
                                maximum=backend_info.get("max_size", 1536) if _backend else 1024,
                                value=backend_info.get("default_size", 512) if _backend else 512,
                                step=64,
                            )
                            height_t2i = gr.Slider(
                                label="ë†’ì´",
                                minimum=512,
                                maximum=backend_info.get("max_size", 1536) if _backend else 1024,
                                value=backend_info.get("default_size", 512) if _backend else 512,
                                step=64,
                            )

                        with gr.Row():
                            num_steps_t2i = gr.Slider(
                                label="ìŠ¤í… ìˆ˜",
                                minimum=2,
                                maximum=10,
                                value=backend_info.get("default_steps", 4) if _backend else 4,
                                step=1,
                                info=backend_info.get("step_info", "") if _backend else "",
                            )
                            seed_t2i = gr.Number(
                                label="ì‹œë“œ",
                                value=-1,
                                precision=0,
                                info="-1 = ëœë¤",
                            )

                        save_image_t2i = gr.Checkbox(
                            label="ì´ë¯¸ì§€ ìë™ ì €ì¥",
                            value=True,
                            info=f"ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR.absolute()}",
                        )

                        with gr.Row():
                            generate_btn_t2i = gr.Button(
                                "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±",
                                variant="primary",
                                size="lg",
                                interactive=_backend is not None,
                                scale=3,
                            )
                            cancel_btn_t2i = gr.Button(
                                "â¹ï¸ ì·¨ì†Œ",
                                variant="stop",
                                size="lg",
                                scale=1,
                            )

                    with gr.Column(scale=1):
                        # ì¶œë ¥ ì„¹ì…˜
                        output_image_t2i = gr.Image(
                            label="ìƒì„±ëœ ì´ë¯¸ì§€",
                            type="pil",
                            height=512,
                        )
                        status_t2i = gr.Textbox(
                            label="ìƒíƒœ",
                            interactive=False,
                        )

                # ì˜ˆì œ
                gr.Examples(
                    examples=[
                        ["A majestic mountain landscape at sunset with snow-capped peaks and a crystal clear lake reflection"],
                        ["ê·€ì—¬ìš´ í•˜ì–€ ê³ ì–‘ì´ê°€ ì°½ê°€ì—ì„œ ë‚®ì ì„ ìê³  ìˆëŠ” ëª¨ìŠµ, ë”°ëœ»í•œ í–‡ì‚´"],
                        ["Cyberpunk city street at night, neon lights, rain reflections, cinematic atmosphere"],
                        ["í•œë³µì„ ì…ì€ ì—¬ì„±ì´ ë²šê½ƒ ë‚˜ë¬´ ì•„ë˜ ì„œ ìˆëŠ” ë™ì–‘í™” ìŠ¤íƒ€ì¼"],
                        ["Delicious Korean bibimbap in a stone pot, food photography, top view"],
                    ],
                    inputs=[prompt_t2i],
                    label="ì˜ˆì œ í”„ë¡¬í”„íŠ¸",
                )

            # ============================================================
            # Tab 2: Image-to-Image
            # ============================================================
            with gr.TabItem("ğŸ–¼ï¸ Image-to-Image"):
                # MLX ê²½ê³  ë©”ì‹œì§€
                if _backend == "mlx":
                    gr.HTML("""
                    <div class="warning-box">
                        <p>âš ï¸ MLX ë°±ì—”ë“œëŠ” Image-to-Imageë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p>
                        <p>PyTorch ë°±ì—”ë“œ (CUDA/MPS/CPU)ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.</p>
                    </div>
                    """)

                with gr.Row():
                    with gr.Column(scale=1):
                        # ì…ë ¥ ì´ë¯¸ì§€
                        init_image = gr.Image(
                            label="ì…ë ¥ ì´ë¯¸ì§€",
                            type="pil",
                            height=256,
                        )

                        # í”„ë¡¬í”„íŠ¸
                        prompt_i2i = gr.Textbox(
                            label="í”„ë¡¬í”„íŠ¸",
                            placeholder="ì´ë¯¸ì§€ë¥¼ ì–´ë–»ê²Œ ë³€í˜•í• ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”...",
                            lines=3,
                            max_lines=6,
                        )

                        # Strength ìŠ¬ë¼ì´ë”
                        strength = gr.Slider(
                            label="ë³€í˜• ê°•ë„ (Strength)",
                            minimum=0.1,
                            maximum=1.0,
                            value=0.6,
                            step=0.05,
                            info="0.1 = ì›ë³¸ ìœ ì§€, 1.0 = ì™„ì „ ë³€í˜•",
                        )

                        with gr.Row():
                            num_steps_i2i = gr.Slider(
                                label="ìŠ¤í… ìˆ˜",
                                minimum=2,
                                maximum=10,
                                value=backend_info.get("default_steps", 6) if _backend else 6,
                                step=1,
                            )
                            seed_i2i = gr.Number(
                                label="ì‹œë“œ",
                                value=-1,
                                precision=0,
                                info="-1 = ëœë¤",
                            )

                        save_image_i2i = gr.Checkbox(
                            label="ì´ë¯¸ì§€ ìë™ ì €ì¥",
                            value=True,
                            info=f"ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR.absolute()}",
                        )

                        with gr.Row():
                            generate_btn_i2i = gr.Button(
                                "ğŸ–¼ï¸ ì´ë¯¸ì§€ ë³€í˜•" if i2i_supported else "ğŸ–¼ï¸ ì´ë¯¸ì§€ ë³€í˜• (MLX ë¯¸ì§€ì›)",
                                variant="primary",
                                size="lg",
                                interactive=i2i_supported,
                                scale=3,
                            )
                            cancel_btn_i2i = gr.Button(
                                "â¹ï¸ ì·¨ì†Œ",
                                variant="stop",
                                size="lg",
                                scale=1,
                            )

                    with gr.Column(scale=1):
                        # ì¶œë ¥ ì„¹ì…˜
                        output_image_i2i = gr.Image(
                            label="ë³€í˜•ëœ ì´ë¯¸ì§€",
                            type="pil",
                            height=512,
                        )
                        status_i2i = gr.Textbox(
                            label="ìƒíƒœ",
                            interactive=False,
                        )

        # ============================================================
        # ì´ë²¤íŠ¸ ì—°ê²°
        # ============================================================

        # Text-to-Image ì´ë²¤íŠ¸
        generate_btn_t2i.click(
            fn=generate_image,
            inputs=[prompt_t2i, width_t2i, height_t2i, num_steps_t2i, seed_t2i, save_image_t2i],
            outputs=[output_image_t2i, status_t2i],
        )

        prompt_t2i.submit(
            fn=generate_image,
            inputs=[prompt_t2i, width_t2i, height_t2i, num_steps_t2i, seed_t2i, save_image_t2i],
            outputs=[output_image_t2i, status_t2i],
        )

        cancel_btn_t2i.click(
            fn=cancel_generation,
            inputs=[],
            outputs=[status_t2i],
        )

        # Image-to-Image ì´ë²¤íŠ¸
        generate_btn_i2i.click(
            fn=generate_image_i2i,
            inputs=[prompt_i2i, init_image, strength, num_steps_i2i, seed_i2i, save_image_i2i],
            outputs=[output_image_i2i, status_i2i],
        )

        cancel_btn_i2i.click(
            fn=cancel_generation,
            inputs=[],
            outputs=[status_i2i],
        )

        gr.HTML("""
        <div class="footer">
            <p>Powered by <a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo" target="_blank">Z-Image-Turbo</a> |
            <a href="https://github.com/filipstrand/mflux" target="_blank">MFLUX</a> (MLX) |
            <a href="https://huggingface.co/docs/diffusers" target="_blank">Diffusers</a> (PyTorch)</p>
        </div>
        """)

    return app


if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ¨ Z-Vision ì‹œì‘")
    print("=" * 50)

    backend = detect_backend()
    if backend:
        info = get_backend_info(backend)
        print(f"{info['emoji']} ê°ì§€ëœ ë°±ì—”ë“œ: {info['name']}")
    else:
        print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ ì—†ìŒ")

    app = create_ui()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        theme=gr.themes.Soft(),
        css="""
        .title { text-align: center; margin-bottom: 1rem; }
        .title .backend-info { font-size: 0.9em; color: #666; margin-top: 0.5rem; }
        .footer { text-align: center; margin-top: 1rem; opacity: 0.7; }
        .error-box { background: #fee; border: 1px solid #fcc; padding: 1rem; border-radius: 8px; margin: 1rem 0; text-align: center; }
        .warning-box { background: #fff3cd !important; border: 1px solid #ffc107 !important; padding: 1rem; border-radius: 8px; margin: 1rem 0; text-align: center; color: #856404 !important; }
        .warning-box p { color: #856404 !important; }
        """,
        head="<title>Z-Vision</title>",
    )
