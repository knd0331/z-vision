"""
Z-Vision - Z-Image-Turbo ê¸°ë°˜ ì´ë¯¸ì§€ ìƒì„± ì›¹ UI (í†µí•© ë²„ì „)

ì§€ì› ë°±ì—”ë“œ:
    - MLX (Apple Silicon ìµœì í™”)
    - CUDA (NVIDIA GPU)
    - MPS (Apple Metal - PyTorch)
    - CPU (í´ë°±)

ì‚¬ìš©ë²•:
    python app.py

ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:7860 ì ‘ì†
"""

import os
import platform
import time
from datetime import datetime
from pathlib import Path

import gradio as gr
from PIL import Image

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
OUTPUT_DIR = Path("outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

# LoRA ì„¤ì •
LORA_DIR = Path("loras")
LORA_DIR.mkdir(exist_ok=True)

# ë‹¤ì¤‘ LoRA ì„¤ì •: [(íŒŒì¼ëª…, ê°•ë„), ...]
# ì˜ˆ: [("zy_CinematicShot_zit", 1.0), ("anime_style", 0.5)]
# ë¹ˆ ë¦¬ìŠ¤íŠ¸ = LoRA ë¯¸ì‚¬ìš©
LORA_CONFIG = [
    ("zy_CinematicShot_zit", 1.0),
    # ("another_lora", 0.5),  # ì£¼ì„ í•´ì œí•˜ì—¬ ì¶”ê°€
]


def get_configured_loras() -> list[tuple[str, float]]:
    """ì„¤ì •ëœ LoRA ëª©ë¡ ë°˜í™˜. [(ê²½ë¡œ, ê°•ë„), ...] í˜•íƒœ."""
    result = []
    for name, scale in LORA_CONFIG:
        path = LORA_DIR / f"{name}.safetensors"
        if path.exists():
            result.append((str(path), scale))
        else:
            print(f"âš ï¸ LoRA íŒŒì¼ ì—†ìŒ: {path}")
    return result


# ì „ì—­ ë³€ìˆ˜
_backend = None
_model = None
_pipeline = None
_img2img_pipeline = None  # Image-to-Image íŒŒì´í”„ë¼ì¸
_is_generating = False
_cancel_requested = False

# LoRA ìƒíƒœ ì¶”ì 
_mlx_lora_configs = None  # MLX ëª¨ë¸ì— ì ìš©ëœ LoRA ì„¤ì • (ì¬ìƒì„± í•„ìš” í™•ì¸ìš©)
_loaded_adapters = []  # Diffusers T2Iì— ë¡œë“œëœ ì–´ëŒ‘í„° ì´ë¦„ë“¤
_loaded_i2i_adapters = []  # Diffusers I2Iì— ë¡œë“œëœ ì–´ëŒ‘í„° ì´ë¦„ë“¤

# ì—…ìŠ¤ì¼€ì¼ëŸ¬
_upscaler = None
_upscaler_device = None


def get_upscaler():
    """Real-ESRGAN ì—…ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ (lazy loading, spandrel ì‚¬ìš©)."""
    global _upscaler, _upscaler_device

    if _upscaler is None:
        import torch
        from spandrel import ImageModelDescriptor, ModelLoader
        from huggingface_hub import hf_hub_download

        # ë””ë°”ì´ìŠ¤ ì„ íƒ (MPS > CUDA > CPU)
        if torch.backends.mps.is_available():
            _upscaler_device = torch.device("mps")
        elif torch.cuda.is_available():
            _upscaler_device = torch.device("cuda")
        else:
            _upscaler_device = torch.device("cpu")

        print(f"ğŸ” ì—…ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”© ì¤‘... (Real-ESRGAN 4x, {_upscaler_device})")

        # HuggingFaceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        model_path = hf_hub_download(
            repo_id="ai-forever/Real-ESRGAN",
            filename="RealESRGAN_x4.pth",
        )

        # Spandrelë¡œ ëª¨ë¸ ë¡œë“œ
        _upscaler = ModelLoader().load_from_file(model_path)
        assert isinstance(_upscaler, ImageModelDescriptor)
        _upscaler.model.to(_upscaler_device).eval()
        print("âœ… ì—…ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”© ì™„ë£Œ!")

    return _upscaler


def upscale_image(image: Image.Image) -> Image.Image:
    """ì´ë¯¸ì§€ ì—…ìŠ¤ì¼€ì¼ (Real-ESRGAN 4x)."""
    import torch
    import numpy as np

    upscaler = get_upscaler()

    # PIL â†’ Tensor (BCHW, 0-1 ë²”ìœ„)
    img_np = np.array(image.convert("RGB")).astype(np.float32) / 255.0
    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0)
    img_tensor = img_tensor.to(_upscaler_device)

    # ì—…ìŠ¤ì¼€ì¼ ì‹¤í–‰
    with torch.no_grad():
        output = upscaler.model(img_tensor)

    # Tensor â†’ PIL
    output = output.squeeze(0).permute(1, 2, 0).cpu().numpy()
    output = (output * 255).clip(0, 255).astype(np.uint8)
    return Image.fromarray(output)


def detect_backend() -> str:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ ë°±ì—”ë“œë¥¼ ê°ì§€."""
    # 1. Apple Siliconì—ì„œ MLX ìš°ì„  ì²´í¬
    if platform.system() == "Darwin" and platform.machine() == "arm64":
        try:
            import mlx.core  # noqa: F401
            from mflux.models.z_image.variants.turbo.z_image_turbo import ZImageTurbo  # noqa: F401
            return "mlx"
        except ImportError:
            pass  # MLX ì—†ìœ¼ë©´ PyTorchë¡œ í´ë°±

    # 2. PyTorch ê¸°ë°˜ ë°±ì—”ë“œ ì²´í¬
    try:
        import torch
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    except ImportError:
        pass

    # 3. ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ None
    return None


def get_backend_info(backend: str) -> dict:
    """ë°±ì—”ë“œë³„ ì„¤ì • ì •ë³´ ë°˜í™˜."""
    configs = {
        "mlx": {
            "name": "MLX (Apple Silicon)",
            "emoji": "ğŸ",
            "default_steps": 4,
            "default_size": 512,
            "max_size": 1536,
            "step_info": "4 ê¶Œì¥ (Turbo ìµœì í™”)",
        },
        "cuda": {
            "name": "CUDA (NVIDIA GPU)",
            "emoji": "ğŸ®",
            "default_steps": 6,
            "default_size": 1024,
            "max_size": 2048,
            "step_info": "6-8 ê¶Œì¥",
        },
        "mps": {
            "name": "MPS (Apple Metal)",
            "emoji": "ğŸ",
            "default_steps": 6,
            "default_size": 512,
            "max_size": 1024,
            "step_info": "6 ê¶Œì¥ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)",
        },
        "cpu": {
            "name": "CPU",
            "emoji": "ğŸ’»",
            "default_steps": 4,
            "default_size": 512,
            "max_size": 768,
            "step_info": "4 ê¶Œì¥ (ëŠë¦¼ ì£¼ì˜)",
        },
    }
    return configs.get(backend, configs["cpu"])


# ============================================================
# MLX Backend
# ============================================================

def get_mlx_model(lora_configs: list[tuple[str, float]] | None = None):
    """MFLUX ZImageTurbo ëª¨ë¸ ë¡œë“œ (lazy loading, ë‹¤ì¤‘ LoRA ì§€ì›)."""
    global _model, _mlx_lora_configs

    # LoRA ì„¤ì • ë³€ê²½ ì‹œ ëª¨ë¸ ì¬ìƒì„± í•„ìš”
    if _model is not None and _mlx_lora_configs != lora_configs:
        old_names = [Path(p).name for p, _ in (_mlx_lora_configs or [])]
        new_names = [Path(p).name for p, _ in (lora_configs or [])]
        print(f"ğŸ”„ LoRA ë³€ê²½ ê°ì§€: {old_names} â†’ {new_names}")
        del _model
        _model = None
        _mlx_lora_configs = None
        import gc
        gc.collect()

    if _model is None:
        from mflux.models.z_image.variants.turbo.z_image_turbo import ZImageTurbo

        if lora_configs:
            lora_paths = [path for path, _ in lora_configs]
            lora_scales = [scale for _, scale in lora_configs]
            lora_names = [Path(p).name for p in lora_paths]
            print(f"ğŸš€ Z-Image-Turbo ëª¨ë¸ ë¡œë”© ì¤‘ (MLX + LoRA: {lora_names})...")
            _model = ZImageTurbo(
                quantize=8,
                lora_paths=lora_paths,
                lora_scales=lora_scales,
            )
            _mlx_lora_configs = lora_configs
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (MLX + 8-bit ì–‘ìí™” + {len(lora_configs)} LoRA)")
        else:
            print("ğŸš€ Z-Image-Turbo ëª¨ë¸ ë¡œë”© ì¤‘ (MLX)...")
            _model = ZImageTurbo(quantize=8)
            _mlx_lora_configs = None
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (MLX + 8-bit ì–‘ìí™”)")

    return _model


def generate_mlx(
    prompt: str, width: int, height: int, num_steps: int, seed: int,
    lora_configs: list[tuple[str, float]] | None = None,
) -> tuple[Image.Image, float]:
    """MLX ë°±ì—”ë“œë¡œ ì´ë¯¸ì§€ ìƒì„± (ë‹¤ì¤‘ LoRA ì§€ì›)."""
    import random
    model = get_mlx_model(lora_configs)

    if seed == -1:
        seed = random.randint(0, 2**32 - 1)

    result = model.generate_image(
        seed=seed,
        prompt=prompt,
        width=width,
        height=height,
        num_inference_steps=num_steps,
    )

    return result.image, result.generation_time, seed


# ============================================================
# PyTorch/Diffusers Backend
# ============================================================

def get_diffusers_pipeline(device: str):
    """Diffusers ZImagePipeline ë¡œë“œ (lazy loading)."""
    global _pipeline
    if _pipeline is None:
        print(f"ğŸš€ Z-Image-Turbo ëª¨ë¸ ë¡œë”© ì¤‘ (PyTorch/{device.upper()})...")
        import torch
        from diffusers import ZImagePipeline

        _pipeline = ZImagePipeline.from_pretrained(
            "Tongyi-MAI/Z-Image-Turbo",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
        )

        if device == "cuda":
            _pipeline.to("cuda")
            print("âœ… CUDA GPUì—ì„œ ì‹¤í–‰")
        elif device == "mps":
            _pipeline.to("mps")
            _pipeline.enable_attention_slicing()
            print("âœ… Apple MPSì—ì„œ ì‹¤í–‰ (attention slicing í™œì„±í™”)")
        else:
            _pipeline.to("cpu")
            _pipeline.enable_attention_slicing()
            print("âš ï¸ CPUì—ì„œ ì‹¤í–‰ (ëŠë¦´ ìˆ˜ ìˆìŒ)")

        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    return _pipeline


def make_progress_callback(num_steps: int, progress_fn=None):
    """Diffusersìš© ì§„í–‰ë¥  ì½œë°± ìƒì„±."""
    def callback(pipeline, step, timestep, callback_kwargs):
        global _cancel_requested
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if progress_fn is not None:
            progress_fn((step + 1) / num_steps, desc=f"Step {step + 1}/{num_steps}")
        
        # ì·¨ì†Œ ìš”ì²­ í™•ì¸
        if _cancel_requested:
            pipeline._interrupt = True
        
        return callback_kwargs
    
    return callback


def generate_diffusers(
    prompt: str, width: int, height: int, num_steps: int, seed: int, device: str,
    progress_fn=None, lora_configs: list[tuple[str, float]] | None = None,
) -> tuple[Image.Image, float]:
    """PyTorch/Diffusers ë°±ì—”ë“œë¡œ ì´ë¯¸ì§€ ìƒì„± (ë‹¤ì¤‘ LoRA ì§€ì›)."""
    global _loaded_adapters
    import torch
    pipe = get_diffusers_pipeline(device)

    # í˜„ì¬ ìš”ì²­ëœ ì–´ëŒ‘í„° ì´ë¦„ê³¼ ê°€ì¤‘ì¹˜ ê³„ì‚°
    requested_adapters = []
    adapter_weights = []
    if lora_configs:
        for path, scale in lora_configs:
            adapter_name = Path(path).stem  # íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
            requested_adapters.append(adapter_name)
            adapter_weights.append(scale)

    # ìƒˆë¡œìš´ LoRA ë¡œë“œ (ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì€ ê²ƒë§Œ)
    for path, scale in (lora_configs or []):
        adapter_name = Path(path).stem
        if adapter_name not in _loaded_adapters:
            print(f"ğŸ¨ LoRA ë¡œë”©: {adapter_name}")
            try:
                pipe.load_lora_weights(path, adapter_name=adapter_name)
                _loaded_adapters.append(adapter_name)
            except Exception as e:
                print(f"âš ï¸ LoRA ë¡œë“œ ì‹¤íŒ¨: {adapter_name} - {e}")

    # ì–´ëŒ‘í„° ì„¤ì • (í™œì„±í™”)
    if requested_adapters:
        print(f"ğŸ¨ LoRA ì ìš©: {list(zip(requested_adapters, adapter_weights))}")
        pipe.set_adapters(requested_adapters, adapter_weights=adapter_weights)
    elif _loaded_adapters:
        # LoRA ì„¤ì •ì´ ë¹„ì–´ìˆìœ¼ë©´ ëª¨ë“  ì–´ëŒ‘í„° ë¹„í™œì„±í™”
        pipe.set_adapters([])

    if seed == -1:
        seed = torch.randint(0, 2**32, (1,)).item()

    generator = torch.Generator(device if device != "cpu" else "cpu").manual_seed(int(seed))

    # ì§„í–‰ë¥  ì½œë°± ìƒì„±
    callback = make_progress_callback(num_steps, progress_fn)

    start_time = time.time()
    result = pipe(
        prompt=prompt,
        width=width,
        height=height,
        num_inference_steps=num_steps,
        guidance_scale=0.0,  # Turbo ëª¨ë¸ì€ 0.0 í•„ìˆ˜
        generator=generator,
        callback_on_step_end=callback,
    )
    gen_time = time.time() - start_time

    return result.images[0], gen_time, seed


# ============================================================
# Image-to-Image Backend (Diffusers Only)
# ============================================================

def get_img2img_pipeline(device: str):
    """Diffusers ZImageImg2ImgPipeline ë¡œë“œ (lazy loading)."""
    global _img2img_pipeline
    if _img2img_pipeline is None:
        print(f"ğŸš€ Z-Image-Turbo Img2Img ëª¨ë¸ ë¡œë”© ì¤‘ (PyTorch/{device.upper()})...")
        import torch
        from diffusers import ZImageImg2ImgPipeline

        _img2img_pipeline = ZImageImg2ImgPipeline.from_pretrained(
            "Tongyi-MAI/Z-Image-Turbo",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
        )

        if device == "cuda":
            _img2img_pipeline.to("cuda")
            print("âœ… CUDA GPUì—ì„œ ì‹¤í–‰ (Img2Img)")
        elif device == "mps":
            _img2img_pipeline.to("mps")
            _img2img_pipeline.enable_attention_slicing()
            print("âœ… Apple MPSì—ì„œ ì‹¤í–‰ (Img2Img, attention slicing í™œì„±í™”)")
        else:
            _img2img_pipeline.to("cpu")
            _img2img_pipeline.enable_attention_slicing()
            print("âš ï¸ CPUì—ì„œ ì‹¤í–‰ (Img2Img, ëŠë¦´ ìˆ˜ ìˆìŒ)")

        print("âœ… Img2Img ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    return _img2img_pipeline


def generate_img2img(
    prompt: str,
    init_image: Image.Image,
    strength: float,
    num_steps: int,
    seed: int,
    device: str,
    progress_fn=None,
    lora_configs: list[tuple[str, float]] | None = None,
) -> tuple[Image.Image, float, int]:
    """PyTorch/Diffusers ë°±ì—”ë“œë¡œ Image-to-Image ìƒì„± (ë‹¤ì¤‘ LoRA ì§€ì›)."""
    global _loaded_i2i_adapters
    import torch
    pipe = get_img2img_pipeline(device)

    # í˜„ì¬ ìš”ì²­ëœ ì–´ëŒ‘í„° ì´ë¦„ê³¼ ê°€ì¤‘ì¹˜ ê³„ì‚°
    requested_adapters = []
    adapter_weights = []
    if lora_configs:
        for path, scale in lora_configs:
            adapter_name = Path(path).stem  # íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
            requested_adapters.append(adapter_name)
            adapter_weights.append(scale)

    # ìƒˆë¡œìš´ LoRA ë¡œë“œ (ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì€ ê²ƒë§Œ)
    for path, scale in (lora_configs or []):
        adapter_name = Path(path).stem
        if adapter_name not in _loaded_i2i_adapters:
            print(f"ğŸ¨ LoRA ë¡œë”© (I2I): {adapter_name}")
            try:
                pipe.load_lora_weights(path, adapter_name=adapter_name)
                _loaded_i2i_adapters.append(adapter_name)
            except Exception as e:
                print(f"âš ï¸ LoRA ë¡œë“œ ì‹¤íŒ¨ (I2I): {adapter_name} - {e}")

    # ì–´ëŒ‘í„° ì„¤ì • (í™œì„±í™”)
    if requested_adapters:
        print(f"ğŸ¨ LoRA ì ìš© (I2I): {list(zip(requested_adapters, adapter_weights))}")
        pipe.set_adapters(requested_adapters, adapter_weights=adapter_weights)
    elif _loaded_i2i_adapters:
        # LoRA ì„¤ì •ì´ ë¹„ì–´ìˆìœ¼ë©´ ëª¨ë“  ì–´ëŒ‘í„° ë¹„í™œì„±í™”
        pipe.set_adapters([])

    if seed == -1:
        seed = torch.randint(0, 2**32, (1,)).item()

    generator = torch.Generator(device if device != "cpu" else "cpu").manual_seed(int(seed))

    # ì§„í–‰ë¥  ì½œë°± ìƒì„±
    callback = make_progress_callback(num_steps, progress_fn)

    start_time = time.time()
    result = pipe(
        prompt=prompt,
        image=init_image,
        strength=strength,
        num_inference_steps=num_steps,
        guidance_scale=0.0,  # Turbo ëª¨ë¸ì€ 0.0 í•„ìˆ˜
        generator=generator,
        callback_on_step_end=callback,
    )
    gen_time = time.time() - start_time

    return result.images[0], gen_time, seed


# ============================================================
# Unified Generation
# ============================================================

def cancel_generation():
    """ìƒì„± ì·¨ì†Œ ìš”ì²­. ë²„íŠ¼ ìƒíƒœë„ í•¨ê»˜ ë°˜í™˜."""
    global _cancel_requested, _is_generating
    if _is_generating:
        _cancel_requested = True
        # ì·¨ì†Œ ìš”ì²­ ì‹œ ë²„íŠ¼ ìƒíƒœ ë³µì› (ìƒì„± ë²„íŠ¼ í‘œì‹œ, ì·¨ì†Œ ë²„íŠ¼ ìˆ¨ê¹€)
        return "â¹ï¸ ì·¨ì†Œ ìš”ì²­ë¨... í˜„ì¬ ìŠ¤í… ì™„ë£Œ í›„ ì¤‘ë‹¨ë©ë‹ˆë‹¤.", gr.update(visible=True), gr.update(visible=False)
    return "ìƒì„± ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.", gr.update(visible=True), gr.update(visible=False)


def unload_model():
    """ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ í•´ì œ."""
    global _model, _pipeline, _img2img_pipeline, _upscaler, _upscaler_device, _is_generating

    if _is_generating:
        return "âš ï¸ ìƒì„± ì¤‘ì—ëŠ” ì–¸ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì·¨ì†Œí•´ì£¼ì„¸ìš”."

    unloaded = []

    # MLX ëª¨ë¸ í•´ì œ
    if _model is not None:
        del _model
        _model = None
        unloaded.append("MLX")

    # Diffusers T2I íŒŒì´í”„ë¼ì¸ í•´ì œ
    if _pipeline is not None:
        del _pipeline
        _pipeline = None
        unloaded.append("T2I Pipeline")

    # Diffusers I2I íŒŒì´í”„ë¼ì¸ í•´ì œ
    if _img2img_pipeline is not None:
        del _img2img_pipeline
        _img2img_pipeline = None
        unloaded.append("I2I Pipeline")

    # ì—…ìŠ¤ì¼€ì¼ëŸ¬ í•´ì œ
    if _upscaler is not None:
        del _upscaler
        _upscaler = None
        _upscaler_device = None
        unloaded.append("Upscaler")

    if not unloaded:
        return "â„¹ï¸ ì–¸ë¡œë“œí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    import gc
    gc.collect()
    
    # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        elif torch.backends.mps.is_available():
            torch.mps.empty_cache()
    except ImportError:
        pass
    
    return f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {', '.join(unloaded)}\nğŸ’¾ ë©”ëª¨ë¦¬ê°€ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤."


# ============================================================
# Unified Generation
# ============================================================

def generate_image(
    prompt: str,
    width: int,
    height: int,
    num_steps: int,
    seed: int,
    save_image: bool,
    upscale: bool = False,
    progress=gr.Progress(),
):
    """í†µí•© ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜ (Generator ë²„ì „ - ë²„íŠ¼ í† ê¸€, LoRA, ì—…ìŠ¤ì¼€ì¼ ì§€ì›)."""
    global _backend, _is_generating, _cancel_requested

    # ë²„íŠ¼ ìƒíƒœ: (ìƒì„± ë²„íŠ¼ visible, ì·¨ì†Œ ë²„íŠ¼ visible)
    BTN_GENERATE = (gr.update(visible=True), gr.update(visible=False))
    BTN_CANCEL = (gr.update(visible=False), gr.update(visible=True))

    if not prompt.strip():
        yield None, "âŒ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", *BTN_GENERATE
        return

    if _backend is None:
        yield None, "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤. PyTorch ë˜ëŠ” MLXë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.", *BTN_GENERATE
        return

    if _is_generating:
        yield None, "âš ï¸ ì´ë¯¸ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê±°ë‚˜ ì·¨ì†Œí•´ì£¼ì„¸ìš”.", *BTN_GENERATE
        return

    # ì „ì—­ LoRA ì„¤ì • ì‚¬ìš©
    lora_configs = get_configured_loras()
    lora_names = [Path(p).stem for p, _ in lora_configs] if lora_configs else []

    try:
        _is_generating = True
        _cancel_requested = False

        # ì¦‰ì‹œ ë²„íŠ¼ ìƒíƒœ ë³€ê²½ (ì·¨ì†Œ ë²„íŠ¼ìœ¼ë¡œ)
        lora_info = f" + LoRA: {lora_names}" if lora_configs else ""
        yield None, f"ğŸš€ ìƒì„± ì‹œì‘...{lora_info}", *BTN_CANCEL

        print(f"ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (backend: {_backend}{lora_info})")
        progress(0, desc="ìƒì„± ì‹œì‘...")

        if _backend == "mlx":
            # MLXëŠ” ì½œë°± ë¯¸ì§€ì›, ë‹¨ìˆœ ì§„í–‰ë¥  í‘œì‹œ
            progress(0.1, desc="MLX ìƒì„± ì¤‘... (ì§„í–‰ë¥  í‘œì‹œ ë¯¸ì§€ì›)")
            image, gen_time, used_seed = generate_mlx(
                prompt, width, height, num_steps, seed,
                lora_configs=lora_configs,
            )
        else:
            # DiffusersëŠ” ì½œë°±ìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
            image, gen_time, used_seed = generate_diffusers(
                prompt, width, height, num_steps, seed, _backend,
                progress_fn=progress, lora_configs=lora_configs,
            )

        # ì·¨ì†Œ í™•ì¸
        if _cancel_requested:
            _is_generating = False
            _cancel_requested = False
            yield None, "â¹ï¸ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.", *BTN_GENERATE
            return

        # ì—…ìŠ¤ì¼€ì¼ ì ìš©
        original_size = image.size if image else None
        if upscale and image is not None:
            progress(0.9, desc="ğŸ” ì—…ìŠ¤ì¼€ì¼ ì¤‘...")
            print("ğŸ” ì—…ìŠ¤ì¼€ì¼ ì¤‘... (Real-ESRGAN 4x)")
            image = upscale_image(image)
            print(f"âœ… ì—…ìŠ¤ì¼€ì¼ ì™„ë£Œ: {original_size} â†’ {image.size}")

        # ìƒíƒœ ë©”ì‹œì§€
        backend_info = get_backend_info(_backend)
        status = f"âœ… ìƒì„± ì™„ë£Œ! ({backend_info['emoji']} {_backend.upper()}, seed: {used_seed}, {gen_time:.1f}ì´ˆ)"
        if upscale and original_size:
            status += f"\nğŸ” ì—…ìŠ¤ì¼€ì¼: {original_size[0]}x{original_size[1]} â†’ {image.size[0]}x{image.size[1]}"

        # ì´ë¯¸ì§€ ì €ì¥
        if save_image and image is not None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            suffix = "_upscaled" if upscale else ""
            filename = OUTPUT_DIR / f"zvision_{timestamp}_{used_seed}{suffix}.png"
            image.save(filename)
            status += f"\nğŸ’¾ ì €ì¥ë¨: {filename}"

        progress(1.0, desc="ì™„ë£Œ!")
        yield image, status, *BTN_GENERATE

    except Exception as e:
        import traceback
        traceback.print_exc()
        yield None, f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", *BTN_GENERATE
    finally:
        _is_generating = False
        _cancel_requested = False


def generate_image_i2i(
    prompt: str,
    init_image: Image.Image,
    strength: float,
    num_steps: int,
    seed: int,
    save_image: bool,
    upscale: bool = False,
    progress=gr.Progress(),
):
    """í†µí•© Image-to-Image ìƒì„± í•¨ìˆ˜ (Generator ë²„ì „ - ë²„íŠ¼ í† ê¸€, LoRA, ì—…ìŠ¤ì¼€ì¼ ì§€ì›)."""
    global _backend, _is_generating, _cancel_requested

    # ë²„íŠ¼ ìƒíƒœ: (ìƒì„± ë²„íŠ¼ visible, ì·¨ì†Œ ë²„íŠ¼ visible)
    BTN_GENERATE = (gr.update(visible=True), gr.update(visible=False))
    BTN_CANCEL = (gr.update(visible=False), gr.update(visible=True))

    if not prompt.strip():
        yield None, "âŒ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", *BTN_GENERATE
        return

    if init_image is None:
        yield None, "âŒ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", *BTN_GENERATE
        return

    if _backend is None:
        yield None, "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.", *BTN_GENERATE
        return

    if _backend == "mlx":
        yield None, "âŒ MLX ë°±ì—”ë“œëŠ” Image-to-Imageë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. PyTorch ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.", *BTN_GENERATE
        return

    if _is_generating:
        yield None, "âš ï¸ ì´ë¯¸ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê±°ë‚˜ ì·¨ì†Œí•´ì£¼ì„¸ìš”.", *BTN_GENERATE
        return

    # ì „ì—­ LoRA ì„¤ì • ì‚¬ìš©
    lora_configs = get_configured_loras()
    lora_names = [Path(p).stem for p, _ in lora_configs] if lora_configs else []

    try:
        _is_generating = True
        _cancel_requested = False

        # ì¦‰ì‹œ ë²„íŠ¼ ìƒíƒœ ë³€ê²½ (ì·¨ì†Œ ë²„íŠ¼ìœ¼ë¡œ)
        lora_info = f" + LoRA: {lora_names}" if lora_configs else ""
        yield None, f"ğŸš€ Img2Img ìƒì„± ì‹œì‘...{lora_info}", *BTN_CANCEL

        print(f"ğŸ–¼ï¸ Image-to-Image ìƒì„± ì¤‘... (backend: {_backend}, strength: {strength}{lora_info})")
        progress(0, desc="Img2Img ìƒì„± ì‹œì‘...")

        image, gen_time, used_seed = generate_img2img(
            prompt, init_image, strength, num_steps, seed, _backend,
            progress_fn=progress, lora_configs=lora_configs,
        )

        # ì·¨ì†Œ í™•ì¸
        if _cancel_requested:
            _is_generating = False
            _cancel_requested = False
            yield None, "â¹ï¸ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.", *BTN_GENERATE
            return

        # ì—…ìŠ¤ì¼€ì¼ ì ìš©
        original_size = image.size if image else None
        if upscale and image is not None:
            progress(0.9, desc="ğŸ” ì—…ìŠ¤ì¼€ì¼ ì¤‘...")
            print("ğŸ” ì—…ìŠ¤ì¼€ì¼ ì¤‘... (Real-ESRGAN 4x)")
            image = upscale_image(image)
            print(f"âœ… ì—…ìŠ¤ì¼€ì¼ ì™„ë£Œ: {original_size} â†’ {image.size}")

        # ìƒíƒœ ë©”ì‹œì§€
        backend_info = get_backend_info(_backend)
        status = f"âœ… Img2Img ì™„ë£Œ! ({backend_info['emoji']} {_backend.upper()}, strength: {strength}, seed: {used_seed}, {gen_time:.1f}ì´ˆ)"
        if upscale and original_size:
            status += f"\nğŸ” ì—…ìŠ¤ì¼€ì¼: {original_size[0]}x{original_size[1]} â†’ {image.size[0]}x{image.size[1]}"

        # ì´ë¯¸ì§€ ì €ì¥
        if save_image and image is not None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            suffix = "_upscaled" if upscale else ""
            filename = OUTPUT_DIR / f"zvision_i2i_{timestamp}_{used_seed}{suffix}.png"
            image.save(filename)
            status += f"\nğŸ’¾ ì €ì¥ë¨: {filename}"

        progress(1.0, desc="ì™„ë£Œ!")
        yield image, status, *BTN_GENERATE

    except Exception as e:
        import traceback
        traceback.print_exc()
        yield None, f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", *BTN_GENERATE
    finally:
        _is_generating = False
        _cancel_requested = False


# ============================================================
# Gradio UI
# ============================================================

def create_ui():
    """Gradio UI ìƒì„±."""
    global _backend
    _backend = detect_backend()
    backend_info = get_backend_info(_backend) if _backend else {"name": "None", "emoji": "âŒ"}

    # Image-to-Image ì§€ì› ì—¬ë¶€ (MLXëŠ” ë¯¸ì§€ì›)
    i2i_supported = _backend is not None and _backend != "mlx"

    with gr.Blocks() as app:

        gr.HTML(f"""
        <div class="title">
            <h1>ğŸ¨ Z-Vision</h1>
            <p>Z-Image-Turbo AI ì´ë¯¸ì§€ ìƒì„±ê¸°</p>
            <p class="backend-info">{backend_info['emoji']} Backend: <strong>{backend_info['name'] if _backend else 'ì—†ìŒ'}</strong></p>
        </div>
        """)

        if _backend is None:
            gr.HTML("""
            <div class="error-box">
                <p>âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤!</p>
                <p>PyTorch ë˜ëŠ” MLXë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.</p>
            </div>
            """)

        with gr.Tabs():
            # ============================================================
            # Tab 1: Text-to-Image
            # ============================================================
            with gr.TabItem("ğŸ¨ Text-to-Image"):
                with gr.Row():
                    with gr.Column(scale=1):
                        # ì…ë ¥ ì„¹ì…˜
                        prompt_t2i = gr.Textbox(
                            label="í”„ë¡¬í”„íŠ¸",
                            placeholder="ìƒì„±í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”...",
                            lines=4,
                            max_lines=8,
                        )

                        with gr.Row():
                            width_t2i = gr.Slider(
                                label="ë„ˆë¹„",
                                minimum=512,
                                maximum=backend_info.get("max_size", 1536) if _backend else 1024,
                                value=backend_info.get("default_size", 512) if _backend else 512,
                                step=64,
                            )
                            height_t2i = gr.Slider(
                                label="ë†’ì´",
                                minimum=512,
                                maximum=backend_info.get("max_size", 1536) if _backend else 1024,
                                value=backend_info.get("default_size", 512) if _backend else 512,
                                step=64,
                            )

                        with gr.Row():
                            num_steps_t2i = gr.Slider(
                                label="ìŠ¤í… ìˆ˜",
                                minimum=2,
                                maximum=10,
                                value=backend_info.get("default_steps", 4) if _backend else 4,
                                step=1,
                                info=backend_info.get("step_info", "") if _backend else "",
                            )
                            seed_t2i = gr.Number(
                                label="ì‹œë“œ",
                                value=-1,
                                precision=0,
                                info="-1 = ëœë¤",
                            )

                        save_image_t2i = gr.Checkbox(
                            label="ì´ë¯¸ì§€ ìë™ ì €ì¥",
                            value=True,
                            info=f"ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR.absolute()}",
                        )

                        upscale_t2i = gr.Checkbox(
                            label="ğŸ” 4x ì—…ìŠ¤ì¼€ì¼ (Real-ESRGAN)",
                            value=False,
                            info="ìƒì„± í›„ 4ë°° í™•ëŒ€ (ì˜ˆ: 1024â†’4096)",
                        )

                        with gr.Row():
                            generate_btn_t2i = gr.Button(
                                "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±",
                                variant="primary",
                                size="lg",
                                interactive=_backend is not None,
                                visible=True,
                            )
                            cancel_btn_t2i = gr.Button(
                                "â¹ï¸ ì·¨ì†Œ",
                                variant="stop",
                                size="lg",
                                visible=False,
                            )

                    with gr.Column(scale=1):
                        # ì¶œë ¥ ì„¹ì…˜
                        output_image_t2i = gr.Image(
                            label="ìƒì„±ëœ ì´ë¯¸ì§€",
                            type="pil",
                            height=512,
                        )
                        status_t2i = gr.Textbox(
                            label="ìƒíƒœ",
                            interactive=False,
                        )

                # ì˜ˆì œ
                gr.Examples(
                    examples=[
                        ["A majestic mountain landscape at sunset with snow-capped peaks and a crystal clear lake reflection"],
                        ["ê·€ì—¬ìš´ í•˜ì–€ ê³ ì–‘ì´ê°€ ì°½ê°€ì—ì„œ ë‚®ì ì„ ìê³  ìˆëŠ” ëª¨ìŠµ, ë”°ëœ»í•œ í–‡ì‚´"],
                        ["Cyberpunk city street at night, neon lights, rain reflections, cinematic atmosphere"],
                        ["í•œë³µì„ ì…ì€ ì—¬ì„±ì´ ë²šê½ƒ ë‚˜ë¬´ ì•„ë˜ ì„œ ìˆëŠ” ë™ì–‘í™” ìŠ¤íƒ€ì¼"],
                        ["Delicious Korean bibimbap in a stone pot, food photography, top view"],
                    ],
                    inputs=[prompt_t2i],
                    label="ì˜ˆì œ í”„ë¡¬í”„íŠ¸",
                )

            # ============================================================
            # Tab 2: Image-to-Image
            # ============================================================
            with gr.TabItem("ğŸ–¼ï¸ Image-to-Image"):
                # MLX ê²½ê³  ë©”ì‹œì§€
                if _backend == "mlx":
                    gr.HTML("""
                    <div class="warning-box">
                        <p>âš ï¸ MLX ë°±ì—”ë“œëŠ” Image-to-Imageë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p>
                        <p>PyTorch ë°±ì—”ë“œ (CUDA/MPS/CPU)ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.</p>
                    </div>
                    """)

                with gr.Row():
                    with gr.Column(scale=1):
                        # ì…ë ¥ ì´ë¯¸ì§€
                        init_image = gr.Image(
                            label="ì…ë ¥ ì´ë¯¸ì§€",
                            type="pil",
                            height=256,
                        )

                        # í”„ë¡¬í”„íŠ¸
                        prompt_i2i = gr.Textbox(
                            label="í”„ë¡¬í”„íŠ¸",
                            placeholder="ì´ë¯¸ì§€ë¥¼ ì–´ë–»ê²Œ ë³€í˜•í• ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”...",
                            lines=3,
                            max_lines=6,
                        )

                        # Strength ìŠ¬ë¼ì´ë”
                        strength = gr.Slider(
                            label="ë³€í˜• ê°•ë„ (Strength)",
                            minimum=0.1,
                            maximum=1.0,
                            value=0.6,
                            step=0.05,
                            info="0.1 = ì›ë³¸ ìœ ì§€, 1.0 = ì™„ì „ ë³€í˜•",
                        )

                        with gr.Row():
                            num_steps_i2i = gr.Slider(
                                label="ìŠ¤í… ìˆ˜",
                                minimum=2,
                                maximum=10,
                                value=backend_info.get("default_steps", 6) if _backend else 6,
                                step=1,
                            )
                            seed_i2i = gr.Number(
                                label="ì‹œë“œ",
                                value=-1,
                                precision=0,
                                info="-1 = ëœë¤",
                            )

                        save_image_i2i = gr.Checkbox(
                            label="ì´ë¯¸ì§€ ìë™ ì €ì¥",
                            value=True,
                            info=f"ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR.absolute()}",
                        )

                        upscale_i2i = gr.Checkbox(
                            label="ğŸ” 4x ì—…ìŠ¤ì¼€ì¼ (Real-ESRGAN)",
                            value=False,
                            info="ìƒì„± í›„ 4ë°° í™•ëŒ€ (ì˜ˆ: 1024â†’4096)",
                        )

                        with gr.Row():
                            generate_btn_i2i = gr.Button(
                                "ğŸ–¼ï¸ ì´ë¯¸ì§€ ë³€í˜•" if i2i_supported else "ğŸ–¼ï¸ ì´ë¯¸ì§€ ë³€í˜• (MLX ë¯¸ì§€ì›)",
                                variant="primary",
                                size="lg",
                                interactive=i2i_supported,
                                visible=True,
                            )
                            cancel_btn_i2i = gr.Button(
                                "â¹ï¸ ì·¨ì†Œ",
                                variant="stop",
                                size="lg",
                                visible=False,
                            )

                    with gr.Column(scale=1):
                        # ì¶œë ¥ ì„¹ì…˜
                        output_image_i2i = gr.Image(
                            label="ë³€í˜•ëœ ì´ë¯¸ì§€",
                            type="pil",
                            height=512,
                        )
                        status_i2i = gr.Textbox(
                            label="ìƒíƒœ",
                            interactive=False,
                        )

        # ============================================================
        # ìœ í‹¸ë¦¬í‹° ì„¹ì…˜ (ëª¨ë¸ ì–¸ë¡œë“œ)
        # ============================================================
        gr.HTML("<hr style='margin: 1.5rem 0; opacity: 0.3;'>")
        
        with gr.Row():
            with gr.Column(scale=1):
                unload_btn = gr.Button(
                    "ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ",
                    variant="secondary",
                    size="sm",
                )
            with gr.Column(scale=3):
                unload_status = gr.Textbox(
                    label="",
                    placeholder="ëª¨ë¸ ì–¸ë¡œë“œ ì‹œ ë©”ëª¨ë¦¬ê°€ í•´ì œë©ë‹ˆë‹¤. ë‹¤ìŒ ìƒì„± ì‹œ ëª¨ë¸ì´ ë‹¤ì‹œ ë¡œë“œë©ë‹ˆë‹¤.",
                    interactive=False,
                    show_label=False,
                )

        # ============================================================
        # ì´ë²¤íŠ¸ ì—°ê²°
        # ============================================================

        # Text-to-Image ì´ë²¤íŠ¸
        generate_btn_t2i.click(
            fn=generate_image,
            inputs=[prompt_t2i, width_t2i, height_t2i, num_steps_t2i, seed_t2i, save_image_t2i, upscale_t2i],
            outputs=[output_image_t2i, status_t2i, generate_btn_t2i, cancel_btn_t2i],
        )

        prompt_t2i.submit(
            fn=generate_image,
            inputs=[prompt_t2i, width_t2i, height_t2i, num_steps_t2i, seed_t2i, save_image_t2i, upscale_t2i],
            outputs=[output_image_t2i, status_t2i, generate_btn_t2i, cancel_btn_t2i],
        )

        cancel_btn_t2i.click(
            fn=cancel_generation,
            inputs=[],
            outputs=[status_t2i, generate_btn_t2i, cancel_btn_t2i],
        )

        # Image-to-Image ì´ë²¤íŠ¸
        generate_btn_i2i.click(
            fn=generate_image_i2i,
            inputs=[prompt_i2i, init_image, strength, num_steps_i2i, seed_i2i, save_image_i2i, upscale_i2i],
            outputs=[output_image_i2i, status_i2i, generate_btn_i2i, cancel_btn_i2i],
        )

        cancel_btn_i2i.click(
            fn=cancel_generation,
            inputs=[],
            outputs=[status_i2i, generate_btn_i2i, cancel_btn_i2i],
        )

        # ëª¨ë¸ ì–¸ë¡œë“œ ì´ë²¤íŠ¸
        unload_btn.click(
            fn=unload_model,
            inputs=[],
            outputs=[unload_status],
        )

        gr.HTML("""
        <div class="footer">
            <p>Powered by <a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo" target="_blank">Z-Image-Turbo</a> |
            <a href="https://github.com/filipstrand/mflux" target="_blank">MFLUX</a> (MLX) |
            <a href="https://huggingface.co/docs/diffusers" target="_blank">Diffusers</a> (PyTorch)</p>
        </div>
        """)

    return app


if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ¨ Z-Vision ì‹œì‘")
    print("=" * 50)

    backend = detect_backend()
    if backend:
        info = get_backend_info(backend)
        print(f"{info['emoji']} ê°ì§€ëœ ë°±ì—”ë“œ: {info['name']}")
    else:
        print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ ì—†ìŒ")

    app = create_ui()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        theme=gr.themes.Soft(),
        css="""
        .title { text-align: center; margin-bottom: 1rem; }
        .title .backend-info { font-size: 0.9em; color: #666; margin-top: 0.5rem; }
        .footer { text-align: center; margin-top: 1rem; opacity: 0.7; }
        .error-box { background: #fee; border: 1px solid #fcc; padding: 1rem; border-radius: 8px; margin: 1rem 0; text-align: center; }
        .warning-box { background: #fff3cd !important; border: 1px solid #ffc107 !important; padding: 1rem; border-radius: 8px; margin: 1rem 0; text-align: center; color: #856404 !important; }
        .warning-box p { color: #856404 !important; }
        """,
        head="<title>Z-Vision</title>",
    )
